import os
# éƒ¨ç½²æ¨¡å¼ - å¦‚æœé‡å‹ä¾èµ–ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
DEPLOYMENT_MODE = os.getenv("DEPLOYMENT_MODE", "false").lower() == "true"

try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    if DEPLOYMENT_MODE:
        # åœ¨éƒ¨ç½²æ¨¡å¼ä¸‹åˆ›å»ºæ¨¡æ‹Ÿçš„cv2æ¨¡å—
        class MockCV2:
            CAP_PROP_FPS = 5
            CAP_PROP_FRAME_COUNT = 7
            CAP_PROP_FRAME_WIDTH = 3
            CAP_PROP_FRAME_HEIGHT = 4
            
            class VideoCapture:
                def __init__(self, path):
                    self.path = path
                
                def isOpened(self):
                    return True
                
                def get(self, prop):
                    if prop == 5:  # CAP_PROP_FPS
                        return 25.0
                    elif prop == 7:  # CAP_PROP_FRAME_COUNT
                        return 1500
                    elif prop == 3:  # CAP_PROP_FRAME_WIDTH
                        return 1920
                    elif prop == 4:  # CAP_PROP_FRAME_HEIGHT
                        return 1080
                    return 0
                
                def read(self):
                    return True, None
                
                def release(self):
                    pass
                
                def set(self, prop, value):
                    return True
        
        cv2 = MockCV2()
        CV2_AVAILABLE = True

import numpy as np
try:
    import librosa
    import soundfile as sf
    from moviepy.editor import VideoFileClip
    MOVIEPY_AVAILABLE = True
except ImportError:
    MOVIEPY_AVAILABLE = False
    
try:
    from sklearn.cluster import KMeans
    from scipy.spatial.distance import euclidean
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    if DEPLOYMENT_MODE:
        # æ¨¡æ‹Ÿsklearn
        class MockKMeans:
            def __init__(self, n_clusters=3, random_state=42):
                self.n_clusters = n_clusters
                self.labels_ = None
                
            def fit(self, data):
                self.labels_ = np.random.randint(0, self.n_clusters, len(data))
                return self
        
        class MockSklearn:
            class cluster:
                KMeans = MockKMeans
        
        import sys
        sys.modules['sklearn'] = MockSklearn()
        sys.modules['sklearn.cluster'] = MockSklearn.cluster()
        
        def euclidean(a, b):
            return np.linalg.norm(np.array(a) - np.array(b))
        
        SKLEARN_AVAILABLE = True

try:
    import matplotlib.pyplot as plt
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    
try:
    from reportlab.lib.pagesizes import letter, A4
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, PageBreak
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib.units import inch
    from reportlab.lib import colors
    from reportlab.platypus import Table, TableStyle
    REPORTLAB_AVAILABLE = True
except ImportError:
    REPORTLAB_AVAILABLE = False

try:
    import whisper
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False
    
import json
from datetime import datetime, timedelta
from pathlib import Path
import logging
from typing import List, Dict, Any, Tuple, Optional
from tqdm import tqdm

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VideoAnalyzer:
    """è§†é¢‘åˆ†æå™¨ - é›†æˆå¤šç§AIåˆ†æåŠŸèƒ½"""
    
    def __init__(self, output_dir: str = "uploads"):
        # ç¡®ä¿ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•
        if not Path(output_dir).is_absolute():
            # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œç¡®ä¿ç›¸å¯¹äºbackendç›®å½•
            backend_dir = Path(__file__).parent.parent
            self.output_dir = backend_dir / output_dir
        else:
            self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–Whisperæ¨¡å‹
        if WHISPER_AVAILABLE:
            try:
                self.whisper_model = whisper.load_model("base")
                logger.info("Whisperæ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.warning(f"Whisperæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.whisper_model = None
        else:
            logger.warning("Whisperæœªå®‰è£…ï¼ŒéŸ³é¢‘è½¬å½•åŠŸèƒ½ä¸å¯ç”¨")
            self.whisper_model = None
        
        # åˆ†æç»“æœ
        self.analysis_results = {}

    def analyze_video(self, video_path: str, task_config: Dict[str, bool], 
                     progress_callback=None, task_id: str = None) -> Dict[str, Any]:
        """
        åˆ†æè§†é¢‘
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            task_config: åˆ†æä»»åŠ¡é…ç½®
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        video_path = Path(video_path)
        if not video_path.exists():
            raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        
        # å¦‚æœæœ‰task_idï¼Œåˆ™ä½¿ç”¨task_idä½œä¸ºæ–‡ä»¶æ ‡è¯†ç¬¦
        if task_id:
            display_video_path = f"uploads/{task_id}.mp4"  # æ ‡å‡†åŒ–çš„æ˜¾ç¤ºè·¯å¾„
        else:
            display_video_path = str(video_path)
        
        results = {
            "video_path": display_video_path,
            "original_video_path": str(video_path),  # ä¿ç•™åŸå§‹è·¯å¾„
            "task_id": task_id,
            "analysis_time": datetime.now().isoformat(),
            "segments": [],
            "transitions": [],
            "transcription": {},
            "summary": {}
        }
        
        try:
            # è·å–è§†é¢‘åŸºæœ¬ä¿¡æ¯
            video_info = self._get_video_info(video_path)
            results["video_info"] = video_info
            
            if progress_callback:
                progress_callback("10", "è·å–è§†é¢‘ä¿¡æ¯å®Œæˆ")
            
            # 1. è§†é¢‘åˆ†å‰²
            if task_config.get("video_segmentation", False):
                logger.info("å¼€å§‹è§†é¢‘åˆ†å‰²...")
                segments = self._segment_video(video_path, progress_callback, task_id)
                results["segments"] = segments
                if progress_callback:
                    progress_callback("30", "è§†é¢‘åˆ†å‰²å®Œæˆ")
            
            # 2. è½¬åœºæ£€æµ‹
            if task_config.get("transition_detection", False):
                logger.info("å¼€å§‹è½¬åœºæ£€æµ‹...")
                transitions = self._detect_transitions(video_path, progress_callback)
                results["transitions"] = transitions
                if progress_callback:
                    progress_callback("50", "è½¬åœºæ£€æµ‹å®Œæˆ")
            
            # 3. éŸ³é¢‘è½¬å½•
            if task_config.get("audio_transcription", False):
                logger.info("å¼€å§‹éŸ³é¢‘è½¬å½•...")
                transcription = self._transcribe_audio(video_path, progress_callback)
                results["transcription"] = transcription
                if progress_callback:
                    progress_callback("70", "éŸ³é¢‘è½¬å½•å®Œæˆ")
            
            # 4. ç”Ÿæˆè„šæœ¬å†…å®¹ï¼ˆåŸºäºè½¬å½•å’Œåˆ†æ®µï¼‰
            if results.get("transcription") and results.get("segments"):
                logger.info("å¼€å§‹ç”Ÿæˆè„šæœ¬...")
                script_content = self._generate_script_content(results, video_path)
                results["script_content"] = script_content
                
                # ä¿å­˜è„šæœ¬æ–‡ä»¶
                if task_id:
                    script_path = self.output_dir / f"{task_id}_script.md"
                    with open(script_path, 'w', encoding='utf-8') as f:
                        f.write(script_content)
                    results["script_file"] = str(script_path)
                    logger.info(f"è„šæœ¬æ–‡ä»¶å·²ç”Ÿæˆ: {script_path}")
                
                if progress_callback:
                    progress_callback("80", "è„šæœ¬ç”Ÿæˆå®Œæˆ")
            
            # 5. ç”Ÿæˆåˆ†ææŠ¥å‘Š
            if task_config.get("report_generation", False):
                logger.info("å¼€å§‹ç”ŸæˆæŠ¥å‘Š...")
                report_path = self._generate_report(results, video_path)
                results["report_path"] = str(report_path)
                if progress_callback:
                    progress_callback("90", "æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            
            if progress_callback:
                progress_callback("100", "åˆ†æå®Œæˆ")
                
            logger.info("è§†é¢‘åˆ†æå®Œæˆ")
            return results
            
        except Exception as e:
            logger.error(f"è§†é¢‘åˆ†æå¤±è´¥: {e}")
            raise
    
    def _get_video_info(self, video_path: Path) -> Dict[str, Any]:
        """è·å–è§†é¢‘åŸºæœ¬ä¿¡æ¯"""
        try:
            if MOVIEPY_AVAILABLE:
                with VideoFileClip(str(video_path)) as clip:
                    return {
                        "duration": clip.duration,
                        "fps": clip.fps,
                        "size": clip.size,
                        "width": clip.w,
                        "height": clip.h,
                        "audio_fps": clip.audio.fps if clip.audio else None
                    }
            else:
                # ä½¿ç”¨OpenCVè·å–è§†é¢‘ä¿¡æ¯
                cap = cv2.VideoCapture(str(video_path))
                if not cap.isOpened():
                    return {}
                    
                fps = cap.get(cv2.CAP_PROP_FPS)
                frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)
                width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                duration = frame_count / fps if fps > 0 else 0
                
                cap.release()
                
                return {
                    "duration": duration,
                    "fps": fps,
                    "size": [width, height],
                    "width": width,
                    "height": height,
                    "audio_fps": 44100
                }
        except Exception as e:
            logger.error(f"è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def _segment_video(self, video_path: Path, progress_callback=None, task_id: str = None) -> List[Dict]:
        """è§†é¢‘åˆ†å‰² - åŸºäºåœºæ™¯å˜åŒ–"""
        segments = []
        
        print(f"ğŸ” AIåˆ†æå™¨ _segment_video è¢«è°ƒç”¨ï¼è§†é¢‘è·¯å¾„: {video_path}")
        
        # éƒ¨ç½²æ¨¡å¼ä¸‹è¿”å›æ¨¡æ‹Ÿæ•°æ®
        if DEPLOYMENT_MODE and not CV2_AVAILABLE:
            logger.info("éƒ¨ç½²æ¨¡å¼ï¼šè¿”å›æ¨¡æ‹Ÿè§†é¢‘åˆ†æ®µæ•°æ®")
            if progress_callback:
                progress_callback("25", "ç”Ÿæˆæ¨¡æ‹Ÿåˆ†æ®µæ•°æ®")
            
            # ç”Ÿæˆ3ä¸ªæ¨¡æ‹Ÿç‰‡æ®µ
            mock_segments = [
                {
                    "segment_id": 1,
                    "start_time": 0.0,
                    "end_time": 30.0,
                    "duration": 30.0,
                    "scene_type": "å¼€åœºä»‹ç»",
                    "frame_count": 750,
                    "composition_analysis": "ä¸­å¿ƒæ„å›¾ï¼Œä¸»ä½“çªå‡ºï¼ŒèƒŒæ™¯ç®€æ´",
                    "camera_movement": "å›ºå®šé•œå¤´ï¼Œå¹³ç¨³æ‹æ‘„",
                    "theme_analysis": "å±•ç¤ºå¼€åœºå†…å®¹ï¼Œæ°›å›´è½»æ¾",
                    "critical_review": "æ­¤ç‰‡æ®µä½œä¸ºå¼€åœºï¼Œæœ‰æ•ˆå»ºç«‹äº†æ•´ä½“æ°›å›´",
                    "transcript_text": "",
                    "thumbnail_url": None,
                    "gif_url": None
                },
                {
                    "segment_id": 2,
                    "start_time": 30.0,
                    "end_time": 90.0,
                    "duration": 60.0,
                    "scene_type": "ä¸»è¦å†…å®¹",
                    "frame_count": 1500,
                    "composition_analysis": "ä¸‰åˆ†æ³•æ„å›¾ï¼Œå±‚æ¬¡ä¸°å¯Œ",
                    "camera_movement": "ç¼“æ…¢æ¨è¿›ï¼Œå¢å¼ºå‚ä¸æ„Ÿ",
                    "theme_analysis": "æ·±å…¥å±•ç¤ºæ ¸å¿ƒå†…å®¹ï¼Œä¿¡æ¯å¯†é›†",
                    "critical_review": "æ­¤ç‰‡æ®µæ˜¯æ•´ä¸ªè§†é¢‘çš„é‡ç‚¹ï¼Œä¿¡æ¯ä¼ è¾¾æ•ˆæœè‰¯å¥½",
                    "transcript_text": "",
                    "thumbnail_url": None,
                    "gif_url": None
                },
                {
                    "segment_id": 3,
                    "start_time": 90.0,
                    "end_time": 120.0,
                    "duration": 30.0,
                    "scene_type": "ç»“å°¾æ€»ç»“",
                    "frame_count": 750,
                    "composition_analysis": "å¯¹ç§°æ„å›¾ï¼Œå¹³è¡¡ç¨³å®š",
                    "camera_movement": "é™æ€é•œå¤´ï¼Œå¼ºè°ƒç¨³å®šæ„Ÿ",
                    "theme_analysis": "æ€»ç»“æ€§å†…å®¹ï¼Œå›é¡¾è¦ç‚¹",
                    "critical_review": "æ­¤ç‰‡æ®µå¾ˆå¥½åœ°æ€»ç»“äº†å‰é¢çš„å†…å®¹ï¼Œå½¢æˆå®Œæ•´é—­ç¯",
                    "transcript_text": "",
                    "thumbnail_url": None,
                    "gif_url": None
                }
            ]
            return mock_segments
        
        try:
            cap = cv2.VideoCapture(str(video_path))
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            # ç”¨äºå­˜å‚¨å¸§çš„ç‰¹å¾
            frame_features = []
            timestamps = []
            
            # æ¯ç§’é‡‡æ ·ä¸€å¸§è¿›è¡Œåˆ†æ
            sample_interval = max(1, int(fps))
            
            frame_idx = 0
            processed_frames = 0
            total_samples = frame_count // sample_interval
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                if frame_idx % sample_interval == 0:
                    # æå–å¸§ç‰¹å¾ (é¢œè‰²ç›´æ–¹å›¾)
                    feature = self._extract_frame_features(frame)
                    frame_features.append(feature)
                    timestamps.append(frame_idx / fps)
                    
                    processed_frames += 1
                    if progress_callback and processed_frames % 10 == 0:
                        progress = 10 + (processed_frames / total_samples) * 15  # 10-25%
                        progress_callback(f"{progress:.0f}", f"åˆ†æå¸§ {processed_frames}/{total_samples}")
                
                frame_idx += 1
            
            cap.release()
            
            if len(frame_features) < 2:
                return segments
            
            # ä½¿ç”¨K-meansèšç±»è¿›è¡Œåœºæ™¯åˆ†å‰²
            n_clusters = min(10, len(frame_features) // 5)  # è‡ªé€‚åº”èšç±»æ•°é‡
            if n_clusters > 1:
                kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                labels = kmeans.fit_predict(frame_features)
                
                # æ‰¾åˆ°åœºæ™¯è¾¹ç•Œ
                scene_changes = [0]
                for i in range(1, len(labels)):
                    if labels[i] != labels[i-1]:
                        scene_changes.append(i)
                scene_changes.append(len(labels) - 1)
                
                # ç”Ÿæˆåˆ†å‰²ç»“æœ
                for i in range(len(scene_changes) - 1):
                    start_idx = scene_changes[i]
                    end_idx = scene_changes[i + 1]
                    
                    # è·å–ä»£è¡¨æ€§å¸§è¿›è¡Œè¯¦ç»†åˆ†æ
                    mid_frame_idx = (start_idx + end_idx) // 2
                    cap.set(cv2.CAP_PROP_POS_FRAMES, mid_frame_idx * sample_interval)
                    ret, representative_frame = cap.read()
                    
                    # è¿›è¡Œè¯¦ç»†åˆ†æ
                    detailed_analysis = self._analyze_frame_details(representative_frame, timestamps[start_idx], timestamps[end_idx])
                    
                    segment = {
                        "segment_id": i + 1,
                        "start_time": timestamps[start_idx],
                        "end_time": timestamps[end_idx],
                        "duration": timestamps[end_idx] - timestamps[start_idx],
                        "scene_type": f"åœºæ™¯ {i + 1}",
                        "frame_count": (end_idx - start_idx) * sample_interval,
                        # æ–°å¢è¯¦ç»†åˆ†æå­—æ®µ
                        "composition_analysis": detailed_analysis["composition"],
                        "camera_movement": detailed_analysis["camera_movement"],
                        "theme_analysis": detailed_analysis["theme"],
                        "critical_review": detailed_analysis["review"],
                        "transcript_text": ""  # å°†åœ¨åé¢æ·»åŠ è½¬å½•æ–‡æœ¬
                    }
                    
                    # ç”Ÿæˆç¼©ç•¥å›¾å’ŒGIF
                    thumbnail_url = None
                    gif_url = None
                    try:
                        if task_id:  # åªæœ‰åœ¨æœ‰task_idæ—¶æ‰ç”Ÿæˆ
                            # åˆ›å»ºæ–°çš„è§†é¢‘æ•è·å¯¹è±¡ç”¨äºç¼©ç•¥å›¾ç”Ÿæˆ
                            thumbnail_cap = cv2.VideoCapture(str(video_path))
                            thumbnail_url = self._generate_segment_thumbnail(thumbnail_cap, timestamps[start_idx], fps, task_id, i + 1)
                            thumbnail_cap.release()
                            
                            gif_url = self._generate_segment_gif(video_path, timestamps[start_idx], timestamps[end_idx], task_id, i + 1)
                    except Exception as e:
                        logger.warning(f"ç”Ÿæˆç‰‡æ®µ{i+1}ç¼©ç•¥å›¾/GIFå¤±è´¥: {e}")
                    
                    segment["thumbnail_url"] = thumbnail_url
                    segment["gif_url"] = gif_url
                    
                    segments.append(segment)
                    print(f"ğŸ¬ AIåˆ†æå™¨ç”Ÿæˆç‰‡æ®µ: {segment['segment_id']}, æ—¶é•¿: {segment['duration']:.2f}s, åœºæ™¯ç±»å‹: {segment['scene_type']}")
            
            logger.info(f"è§†é¢‘åˆ†å‰²å®Œæˆï¼Œå…±è¯†åˆ« {len(segments)} ä¸ªåœºæ™¯")
            return segments
            
        except Exception as e:
            logger.error(f"è§†é¢‘åˆ†å‰²å¤±è´¥: {e}")
            return []
    
    def _extract_frame_features(self, frame) -> np.ndarray:
        """æå–å¸§ç‰¹å¾"""
        # è½¬æ¢åˆ°HSVé¢œè‰²ç©ºé—´
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        
        # è®¡ç®—é¢œè‰²ç›´æ–¹å›¾
        hist_h = cv2.calcHist([hsv], [0], None, [50], [0, 180])
        hist_s = cv2.calcHist([hsv], [1], None, [50], [0, 256])
        hist_v = cv2.calcHist([hsv], [2], None, [50], [0, 256])
        
        # å½’ä¸€åŒ–å¹¶è¿æ¥ç‰¹å¾
        hist_h = hist_h.flatten() / hist_h.sum()
        hist_s = hist_s.flatten() / hist_s.sum()
        hist_v = hist_v.flatten() / hist_v.sum()
        
        feature = np.concatenate([hist_h, hist_s, hist_v])
        return feature
    
    def _analyze_frame_details(self, frame, start_time: float, end_time: float) -> Dict[str, str]:
        """åˆ†æå¸§çš„è¯¦ç»†ä¿¡æ¯ï¼ˆæ„å›¾ã€è¿é•œã€ä¸»é¢˜ã€ç®€è¯„ï¼‰"""
        if frame is None:
            return {
                "composition": "æ— æ³•åˆ†æç”»é¢æ„å›¾",
                "camera_movement": "æ— æ³•åˆ†æé•œå¤´è¿åŠ¨",
                "theme": "æ— æ³•åˆ†æä¸»é¢˜å†…å®¹",
                "review": "æ— æ³•ç”Ÿæˆè¯„ä»·"
            }
        
        try:
            # è·å–å¸§çš„åŸºæœ¬ä¿¡æ¯
            height, width = frame.shape[:2]
            duration = end_time - start_time
            
            # æ„å›¾åˆ†æï¼ˆåŸºäºå›¾åƒç‰¹å¾ï¼‰
            composition = self._analyze_composition(frame, width, height)
            
            # è¿é•œåˆ†æï¼ˆåŸºäºæ—¶é•¿å’Œç”»é¢ç‰¹å¾ï¼‰
            camera_movement = self._analyze_camera_movement(frame, duration)
            
            # ä¸»é¢˜åˆ†æï¼ˆåŸºäºç”»é¢å†…å®¹ï¼‰
            theme = self._analyze_theme(frame, width, height)
            
            # ç®€è¯„ï¼ˆç»¼åˆåˆ†æï¼‰
            review = self._generate_critical_review(composition, camera_movement, theme, duration)
            
            return {
                "composition": composition,
                "camera_movement": camera_movement,
                "theme": theme,
                "review": review
            }
            
        except Exception as e:
            logger.warning(f"è¯¦ç»†åˆ†æå¤±è´¥: {e}")
            return {
                "composition": "åŸºç¡€ç”»é¢æ„å›¾ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ",
                "camera_movement": "é™æ€é•œå¤´æˆ–è½»å¾®è¿åŠ¨",
                "theme": "å†…å®¹ä¸»é¢˜æœ‰å¾…è¯†åˆ«",
                "review": "é•œå¤´å…·æœ‰ä¸€å®šçš„è§†è§‰è¡¨ç°åŠ›"
            }
    
    def _analyze_composition(self, frame, width: int, height: int) -> str:
        """åˆ†æç”»é¢æ„å›¾"""
        # è®¡ç®—ç”»é¢äº®åº¦åˆ†å¸ƒ
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        brightness = np.mean(gray)
        
        # è®¡ç®—å¯¹æ¯”åº¦
        contrast = np.std(gray)
        
        # æ£€æµ‹è¾¹ç¼˜å¯†åº¦
        edges = cv2.Canny(gray, 50, 150)
        edge_density = np.sum(edges > 0) / (width * height)
        
        # åˆ†ææ„å›¾ç±»å‹
        if edge_density > 0.15:
            if contrast > 60:
                return "å¤æ‚æ„å›¾ï¼Œç”»é¢å±‚æ¬¡ä¸°å¯Œï¼Œå…·æœ‰å¼ºçƒˆçš„è§†è§‰å†²å‡»åŠ›"
            else:
                return "ç²¾ç»†æ„å›¾ï¼Œç»†èŠ‚ä¸°å¯Œï¼Œç”»é¢å†…å®¹å……å®"
        elif brightness > 180:
            return "æ˜äº®æ„å›¾ï¼Œç”»é¢ç®€æ´æ˜äº†ï¼Œè‰²è°ƒåäº®"
        elif brightness < 80:
            return "æš—è°ƒæ„å›¾ï¼Œè¥é€ ç¥ç§˜æˆ–æ²‰ç¨³çš„è§†è§‰æ°›å›´"
        else:
            return "å‡è¡¡æ„å›¾ï¼Œç”»é¢å’Œè°ï¼Œæ˜æš—åˆ†å¸ƒé€‚ä¸­"
    
    def _analyze_camera_movement(self, frame, duration: float) -> str:
        """åˆ†æé•œå¤´è¿åŠ¨"""
        # åŸºäºæ—¶é•¿åˆ¤æ–­é•œå¤´ç±»å‹
        if duration < 2:
            return "å¿«åˆ‡é•œå¤´ï¼ŒèŠ‚å¥ç´§å‡‘ï¼Œé€‚åˆå±•ç°åŠ¨æ„Ÿæˆ–ç´§å¼ æ„Ÿ"
        elif duration < 5:
            return "æ ‡å‡†é•œå¤´é•¿åº¦ï¼Œå™äº‹èŠ‚å¥é€‚ä¸­ï¼Œè§‚ä¼—å®¹æ˜“æ¥å—"
        elif duration < 10:
            return "æ…¢èŠ‚å¥é•œå¤´ï¼Œç»™è§‚ä¼—å……åˆ†çš„è§‚å¯Ÿå’Œæ€è€ƒæ—¶é—´"
        else:
            return "é•¿é•œå¤´ï¼Œæ·±åº¦å™äº‹æˆ–æƒ…æ„Ÿæ¸²æŸ“ï¼Œå…·æœ‰è‰ºæœ¯è¡¨ç°åŠ›"
    
    def _analyze_theme(self, frame, width: int, height: int) -> str:
        """åˆ†æä¸»é¢˜å†…å®¹"""
        # åˆ†æè‰²å½©ä¸»é¢˜
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        
        # è®¡ç®—ä¸»è¦è‰²è°ƒ
        hue_hist = cv2.calcHist([hsv], [0], None, [180], [0, 180])
        dominant_hue = np.argmax(hue_hist)
        
        # è®¡ç®—é¥±å’Œåº¦
        saturation = np.mean(hsv[:, :, 1])
        
        # åŸºäºè‰²å½©åˆ†æä¸»é¢˜
        if dominant_hue < 30 or dominant_hue > 150:  # çº¢è‰²ç³»
            if saturation > 100:
                return "çƒ­æƒ…ä¸»é¢˜ï¼Œçº¢è‰²è°ƒè¥é€ æ¸©æš–æˆ–æ¿€çƒˆçš„æƒ…æ„Ÿæ°›å›´"
            else:
                return "æ¸©å’Œä¸»é¢˜ï¼Œåæš–è‰²è°ƒï¼Œç»™äººäº²åˆ‡æ„Ÿ"
        elif 30 <= dominant_hue <= 90:  # é»„ç»¿è‰²ç³»
            return "è‡ªç„¶ä¸»é¢˜ï¼Œç»¿è‰²è°ƒè¥é€ æ¸…æ–°æˆ–ç”Ÿæœºå‹ƒå‹ƒçš„æ„Ÿè§‰"
        elif 90 < dominant_hue <= 130:  # è“è‰²ç³»
            return "å†·é™ä¸»é¢˜ï¼Œè“è‰²è°ƒè¥é€ å®é™æˆ–ç†æ€§çš„æ°›å›´"
        else:
            if saturation > 80:
                return "æ´»è·ƒä¸»é¢˜ï¼Œè‰²å½©é¥±å’Œåº¦é«˜ï¼Œè§†è§‰æ•ˆæœçªå‡º"
            else:
                return "ä¸­æ€§ä¸»é¢˜ï¼Œè‰²å½©å¹³å’Œï¼Œå†…å®¹å¯¼å‘å‹ç”»é¢"
    
    def _generate_critical_review(self, composition: str, camera_movement: str, theme: str, duration: float) -> str:
        """ç”Ÿæˆç®€è¯„"""
        # åŸºäºå„é¡¹åˆ†æç”Ÿæˆç»¼åˆè¯„ä»·
        reviews = []
        
        if "å¤æ‚" in composition or "ä¸°å¯Œ" in composition:
            reviews.append("ç”»é¢è¡¨ç°åŠ›å¼º")
        
        if duration < 3:
            reviews.append("èŠ‚å¥æ„Ÿçªå‡º")
        elif duration > 8:
            reviews.append("å…·æœ‰æ·±åº¦å™äº‹ä»·å€¼")
        
        if "çƒ­æƒ…" in theme or "æ´»è·ƒ" in theme:
            reviews.append("æƒ…æ„Ÿè¡¨è¾¾ç›´æ¥")
        elif "è‡ªç„¶" in theme or "å†·é™" in theme:
            reviews.append("æ°›å›´è¥é€ åˆ°ä½")
        
        if not reviews:
            reviews.append("å…·æœ‰åŸºç¡€çš„è§†è§‰ä»·å€¼")
        
        base_review = "ã€".join(reviews)
        
        # æ·»åŠ åˆ›ä½œæ‰‹æ³•è¯„ä»·
        if duration > 6 and ("å¤æ‚" in composition or "ä¸°å¯Œ" in composition):
            technique = "è¿ç”¨é•¿é•œå¤´æ·±åº¦æ„å›¾çš„æ‹æ‘„æ‰‹æ³•"
        elif duration < 3:
            technique = "é‡‡ç”¨å¿«èŠ‚å¥å‰ªè¾‘æ‰‹æ³•"
        else:
            technique = "è¿ç”¨æ ‡å‡†çš„å½±åƒå™äº‹æ‰‹æ³•"
        
        return f"æ­¤é•œå¤´{base_review}ï¼Œ{technique}ï¼Œåœ¨æ•´ä½“å™äº‹ä¸­èµ·åˆ°é‡è¦çš„è§†è§‰æ”¯æ’‘ä½œç”¨ã€‚"
    
    def _assign_transcription_to_segments(self, results: Dict[str, Any]):
        """ä¸ºè§†é¢‘ç‰‡æ®µåˆ†é…è½¬å½•æ–‡æœ¬"""
        segments = results.get("segments", [])
        transcription = results.get("transcription", {})
        transcript_segments = transcription.get("segments", [])
        
        if not transcript_segments:
            return
        
        logger.info("å¼€å§‹ä¸ºè§†é¢‘ç‰‡æ®µåˆ†é…è½¬å½•æ–‡æœ¬...")
        
        for segment in segments:
            start_time = segment["start_time"]
            end_time = segment["end_time"]
            
            # æ‰¾åˆ°æ—¶é—´èŒƒå›´å†…çš„è½¬å½•æ–‡æœ¬
            segment_texts = []
            for trans_seg in transcript_segments:
                trans_start = trans_seg["start"]
                trans_end = trans_seg["end"]
                
                # æ£€æŸ¥æ—¶é—´é‡å 
                if trans_end >= start_time and trans_start <= end_time:
                    # æœ‰é‡å ï¼Œæ·»åŠ è¿™æ®µæ–‡æœ¬
                    text = trans_seg["text"].strip()
                    if text:
                        segment_texts.append(text)
            
            # åˆå¹¶æ–‡æœ¬
            segment["transcript_text"] = " ".join(segment_texts) if segment_texts else ""
            
        logger.info("è½¬å½•æ–‡æœ¬åˆ†é…å®Œæˆ")
    
    def _generate_script_content(self, results: Dict[str, Any], video_path: Path) -> str:
        """ç”Ÿæˆå®Œæ•´çš„è„šæœ¬å†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰"""
        segments = results.get("segments", [])
        video_info = results.get("video_info", {})
        transcription = results.get("transcription", {})
        
        # ä¸ºç‰‡æ®µåˆ†é…è½¬å½•æ–‡æœ¬
        self._assign_transcription_to_segments(results)
        
        # ç”ŸæˆMarkdownå†…å®¹
        content = []
        
        # æ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯
        content.append(f"# è§†é¢‘è„šæœ¬åˆ†ææŠ¥å‘Š")
        content.append(f"\n**è§†é¢‘æ–‡ä»¶:** {video_path.name}")
        content.append(f"**æ€»æ—¶é•¿:** {video_info.get('duration', 0):.2f} ç§’")
        content.append(f"**åˆ†ææ—¶é—´:** {results.get('analysis_time', '')[:19]}")
        content.append(f"**ç‰‡æ®µæ€»æ•°:** {len(segments)} ä¸ª")
        content.append("\n---\n")
        
        # å®Œæ•´è½¬å½•æ–‡æœ¬
        if transcription.get("text"):
            content.append("## å®Œæ•´è½¬å½•æ–‡æœ¬\n")
            full_text = transcription["text"].strip()
            # ç®€å•åˆ†æ®µå¤„ç†
            paragraphs = full_text.split("ã€‚")
            formatted_paragraphs = []
            
            current_paragraph = ""
            for sentence in paragraphs:
                sentence = sentence.strip()
                if sentence:
                    current_paragraph += sentence + "ã€‚"
                    # æ¯3-4å¥è¯åˆ†ä¸€æ®µ
                    if len(current_paragraph) > 150 or sentence.endswith(("ï¼Ÿ", "ï¼")):
                        formatted_paragraphs.append(current_paragraph.strip())
                        current_paragraph = ""
            
            if current_paragraph.strip():
                formatted_paragraphs.append(current_paragraph.strip())
            
            for para in formatted_paragraphs:
                if para:
                    content.append(f"{para}\n")
            
            content.append("\n---\n")
        
        # åˆ†æ®µè¯¦ç»†åˆ†æ
        content.append("## åˆ†æ®µè¯¦ç»†åˆ†æ\n")
        
        for i, segment in enumerate(segments, 1):
            content.append(f"### ç‰‡æ®µ {i} ({segment['start_time']:.1f}s - {segment['end_time']:.1f}s)")
            content.append(f"**æ—¶é•¿:** {segment['duration']:.1f} ç§’\n")
            
            # æ–‡æ¡ˆå†…å®¹
            if segment.get("transcript_text"):
                content.append(f"**æ–‡æ¡ˆå†…å®¹:**")
                content.append(f"{segment['transcript_text']}\n")
            
            # æ„å›¾åˆ†æ
            content.append(f"**æ„å›¾åˆ†æ:**")
            content.append(f"{segment.get('composition_analysis', 'æš‚æ— åˆ†æ')}\n")
            
            # è¿é•œåˆ†æ
            content.append(f"**è¿é•œåˆ†æ:**")
            content.append(f"{segment.get('camera_movement', 'æš‚æ— åˆ†æ')}\n")
            
            # ä¸»é¢˜åˆ†æ
            content.append(f"**ä¸»é¢˜åˆ†æ:**")
            content.append(f"{segment.get('theme_analysis', 'æš‚æ— åˆ†æ')}\n")
            
            # ç®€è¯„
            content.append(f"**ä¸“ä¸šç®€è¯„:**")
            content.append(f"{segment.get('critical_review', 'æš‚æ— è¯„ä»·')}\n")
            
            content.append("---\n")
        
        # æ€»ç»“
        content.append("## æ€»ä½“è¯„ä»·\n")
        content.append(self._generate_overall_summary(segments, video_info))
        
        return "\n".join(content)
    
    def _generate_overall_summary(self, segments: List[Dict], video_info: Dict) -> str:
        """ç”Ÿæˆæ€»ä½“è¯„ä»·"""
        total_duration = video_info.get("duration", 0)
        segment_count = len(segments)
        
        if segment_count == 0:
            return "è§†é¢‘åˆ†ææ•°æ®ä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆæ€»ä½“è¯„ä»·ã€‚"
        
        avg_segment_duration = total_duration / segment_count
        
        # åˆ†æèŠ‚å¥ç‰¹ç‚¹
        if avg_segment_duration < 3:
            rhythm = "å¿«èŠ‚å¥å‰ªè¾‘é£æ ¼ï¼Œé•œå¤´åˆ‡æ¢é¢‘ç¹ï¼Œé€‚åˆå±•ç°åŠ¨æ„Ÿå†…å®¹"
        elif avg_segment_duration < 8:
            rhythm = "ä¸­ç­‰èŠ‚å¥ï¼Œå‰ªè¾‘èŠ‚å¥é€‚ä¸­ï¼Œç¬¦åˆè§‚ä¼—è§‚çœ‹ä¹ æƒ¯"
        else:
            rhythm = "æ…¢èŠ‚å¥é£æ ¼ï¼Œæ³¨é‡æ·±åº¦å™äº‹å’Œæƒ…æ„Ÿæ¸²æŸ“"
        
        # åˆ†ææ„å›¾å¤šæ ·æ€§
        composition_types = [seg.get('composition_analysis', '') for seg in segments]
        unique_compositions = len(set(composition_types))
        
        if unique_compositions > segment_count * 0.8:
            composition_variety = "æ„å›¾å˜åŒ–ä¸°å¯Œï¼Œè§†è§‰è¡¨ç°åŠ›å¼º"
        elif unique_compositions > segment_count * 0.5:
            composition_variety = "æ„å›¾æœ‰ä¸€å®šå˜åŒ–ï¼Œè§†è§‰å±‚æ¬¡è¾ƒå¥½"
        else:
            composition_variety = "æ„å›¾ç›¸å¯¹ç»Ÿä¸€ï¼Œé£æ ¼ä¸€è‡´æ€§è¾ƒå¼º"
        
        summary = f"""
æœ¬è§†é¢‘å…±è®¡{total_duration:.1f}ç§’ï¼Œåˆ†ä¸º{segment_count}ä¸ªç‰‡æ®µï¼Œå¹³å‡æ¯ä¸ªç‰‡æ®µ{avg_segment_duration:.1f}ç§’ã€‚

**èŠ‚å¥ç‰¹ç‚¹:** {rhythm}

**è§†è§‰ç‰¹ç‚¹:** {composition_variety}

**æ•´ä½“è¯„ä»·:** è¯¥è§†é¢‘åœ¨é•œå¤´è¯­è¨€è¿ç”¨ä¸Š{'è¡¨ç°å‡ºè‰²' if segment_count > 20 else 'åŸºç¡€è§„èŒƒ'}ï¼Œ{rhythm.split('ï¼Œ')[0]}ï¼Œé€‚åˆ{'çŸ­è§†é¢‘å¹³å°ä¼ æ’­' if avg_segment_duration < 5 else 'æ·±åº¦å†…å®¹å±•ç¤º'}ã€‚è§†é¢‘çš„å‰ªè¾‘æ‰‹æ³•å’Œæ„å›¾å®‰æ’æ˜¾ç¤ºäº†åˆ›ä½œè€…{'è¾ƒé«˜çš„ä¸“ä¸šæ°´å¹³' if unique_compositions > segment_count * 0.6 else 'åŸºç¡€çš„åˆ¶ä½œèƒ½åŠ›'}ã€‚

**å»ºè®®:** {'ä¿æŒå½“å‰çš„å¿«èŠ‚å¥é£æ ¼ï¼Œå¯è€ƒè™‘åœ¨å…³é”®èŠ‚ç‚¹é€‚å½“å»¶é•¿é•œå¤´ä»¥å¢å¼ºè¡¨ç°åŠ›' if avg_segment_duration < 4 else 'å½“å‰èŠ‚å¥é€‚ä¸­ï¼Œå»ºè®®åœ¨æƒ…æ„Ÿé«˜æ½®éƒ¨åˆ†è¿›ä¸€æ­¥ä¼˜åŒ–é•œå¤´è¯­è¨€' if avg_segment_duration < 8 else 'é•¿é•œå¤´è¿ç”¨æ°å½“ï¼Œå»ºè®®åœ¨é€‚å½“ä½ç½®å¢åŠ ä¸€äº›å¿«åˆ‡é•œå¤´ä»¥ä¸°å¯Œè§†è§‰å±‚æ¬¡'}ã€‚
        """.strip()
        
        return summary
    
    def _detect_transitions(self, video_path: Path, progress_callback=None) -> List[Dict]:
        """è½¬åœºæ£€æµ‹"""
        transitions = []
        
        try:
            cap = cv2.VideoCapture(str(video_path))
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            prev_frame = None
            frame_idx = 0
            transition_threshold = 0.25  # è½¬åœºé˜ˆå€¼ï¼ˆé™ä½ä»¥æ£€æµ‹æ›´å¤šè½¬åœºï¼‰
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                if prev_frame is not None:
                    # è®¡ç®—å¸§é—´å·®å¼‚
                    diff = self._calculate_frame_difference(prev_frame, frame)
                    
                    # æ£€æµ‹è½¬åœº
                    if diff > transition_threshold:
                        timestamp = frame_idx / fps
                        transition = {
                            "transition_id": len(transitions) + 1,
                            "timestamp": timestamp,
                            "strength": float(diff),
                            "type": self._classify_transition_type(diff)
                        }
                        transitions.append(transition)
                
                prev_frame = frame.copy()
                frame_idx += 1
                
                if progress_callback and frame_idx % 100 == 0:
                    progress = 30 + (frame_idx / frame_count) * 15  # 30-45%
                    progress_callback(f"{progress:.0f}", f"æ£€æµ‹è½¬åœº {frame_idx}/{frame_count}")
            
            cap.release()
            
            # è¿‡æ»¤è¿‡äºå¯†é›†çš„è½¬åœº
            transitions = self._filter_transitions(transitions)
            
            logger.info(f"è½¬åœºæ£€æµ‹å®Œæˆï¼Œå…±è¯†åˆ« {len(transitions)} ä¸ªè½¬åœº")
            return transitions
            
        except Exception as e:
            logger.error(f"è½¬åœºæ£€æµ‹å¤±è´¥: {e}")
            return []
    
    def _calculate_frame_difference(self, frame1, frame2) -> float:
        """è®¡ç®—ä¸¤å¸§ä¹‹é—´çš„å·®å¼‚"""
        # è½¬æ¢ä¸ºç°åº¦å›¾
        gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)
        
        # è®¡ç®—ç›´æ–¹å›¾
        hist1 = cv2.calcHist([gray1], [0], None, [256], [0, 256])
        hist2 = cv2.calcHist([gray2], [0], None, [256], [0, 256])
        
        # è®¡ç®—ç›¸å…³æ€§
        correlation = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)
        
        # è¿”å›å·®å¼‚å€¼ï¼ˆ1 - ç›¸å…³æ€§ï¼‰
        return 1.0 - correlation
    
    def _classify_transition_type(self, strength: float) -> str:
        """åˆ†ç±»è½¬åœºç±»å‹"""
        if strength > 0.7:
            return "ç¡¬åˆ‡"
        elif strength > 0.5:
            return "æ¸å˜"
        else:
            return "è½»å¾®å˜åŒ–"
    
    def _filter_transitions(self, transitions: List[Dict], min_interval: float = 1.0) -> List[Dict]:
        """è¿‡æ»¤è¿‡äºå¯†é›†çš„è½¬åœº"""
        if not transitions:
            return transitions
        
        filtered = [transitions[0]]
        
        for transition in transitions[1:]:
            last_transition = filtered[-1]
            if transition["timestamp"] - last_transition["timestamp"] >= min_interval:
                filtered.append(transition)
        
        return filtered
    
    def _transcribe_audio(self, video_path: Path, progress_callback=None) -> Dict[str, Any]:
        """éŸ³é¢‘è½¬å½•"""
        if not self.whisper_model:
            logger.warning("Whisperæ¨¡å‹æœªåŠ è½½ï¼Œè·³è¿‡éŸ³é¢‘è½¬å½•")
            return {"error": "Whisperæ¨¡å‹æœªåŠ è½½"}
        
        try:
            # æå–éŸ³é¢‘
            audio_path = self.output_dir / f"{video_path.stem}_audio.wav"
            
            with VideoFileClip(str(video_path)) as video:
                if video.audio is None:
                    return {"error": "è§†é¢‘æ²¡æœ‰éŸ³é¢‘è½¨é“"}
                
                # å¯¼å‡ºéŸ³é¢‘
                video.audio.write_audiofile(str(audio_path), verbose=False, logger=None)
            
            if progress_callback:
                progress_callback("55", "éŸ³é¢‘æå–å®Œæˆ")
            
            # ä½¿ç”¨Whisperè½¬å½•
            logger.info("å¼€å§‹éŸ³é¢‘è½¬å½•...")
            result = self.whisper_model.transcribe(str(audio_path), language="zh")
            
            if progress_callback:
                progress_callback("65", "éŸ³é¢‘è½¬å½•å®Œæˆ")
            
            # æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
            audio_path.unlink()
            
            # å¤„ç†è½¬å½•ç»“æœ
            transcription = {
                "text": result["text"],
                "language": result.get("language", "zh"),
                "segments": []
            }
            
            for segment in result.get("segments", []):
                transcription["segments"].append({
                    "start": segment["start"],
                    "end": segment["end"],
                    "text": segment["text"].strip(),
                    "confidence": segment.get("avg_logprob", 0)
                })
            
            # ç”Ÿæˆå­—å¹•æ–‡ä»¶
            srt_path = self._generate_subtitles(transcription, video_path)
            transcription["subtitle_file"] = str(srt_path)
            
            logger.info(f"éŸ³é¢‘è½¬å½•å®Œæˆï¼Œè¯†åˆ«æ–‡æœ¬é•¿åº¦: {len(transcription['text'])}")
            return transcription
            
        except Exception as e:
            logger.error(f"éŸ³é¢‘è½¬å½•å¤±è´¥: {e}")
            return {"error": str(e)}
    
    def _generate_subtitles(self, transcription: Dict, video_path: Path) -> Path:
        """ç”ŸæˆSRTå­—å¹•æ–‡ä»¶"""
        srt_path = self.output_dir / f"{video_path.stem}_subtitles.srt"
        
        with open(srt_path, 'w', encoding='utf-8') as f:
            for i, segment in enumerate(transcription["segments"]):
                start_time = self._seconds_to_srt_time(segment["start"])
                end_time = self._seconds_to_srt_time(segment["end"])
                
                f.write(f"{i + 1}\n")
                f.write(f"{start_time} --> {end_time}\n")
                f.write(f"{segment['text']}\n\n")
        
        return srt_path
    
    def _seconds_to_srt_time(self, seconds: float) -> str:
        """å°†ç§’æ•°è½¬æ¢ä¸ºSRTæ—¶é—´æ ¼å¼"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millisecs = int((seconds % 1) * 1000)
        
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"
    
    def _generate_report(self, results: Dict, video_path: Path) -> Path:
        """ç”Ÿæˆåˆ†ææŠ¥å‘ŠPDF"""
        from reportlab.pdfbase import pdfmetrics
        from reportlab.pdfbase.ttfonts import TTFont
        from reportlab.lib.fonts import addMapping
        import platform
        
        report_path = self.output_dir / f"{video_path.stem}_analysis_report.pdf"
        
        # æ³¨å†Œä¸­æ–‡å­—ä½“
        try:
            if platform.system() == "Darwin":  # macOS
                font_path = "/System/Library/Fonts/STHeiti Medium.ttc"
                if not Path(font_path).exists():
                    font_path = "/System/Library/Fonts/STHeiti Light.ttc"
                if not Path(font_path).exists():
                    font_path = "/System/Library/Fonts/Helvetica.ttc"
            elif platform.system() == "Windows":
                font_path = "C:/Windows/Fonts/msyh.ttc"  # å¾®è½¯é›…é»‘
                if not Path(font_path).exists():
                    font_path = "C:/Windows/Fonts/simsun.ttc"  # å®‹ä½“
            else:  # Linux
                font_path = "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf"
            
            if Path(font_path).exists():
                pdfmetrics.registerFont(TTFont('ChineseFont', font_path))
                addMapping('ChineseFont', 0, 0, 'ChineseFont')  # normal
                addMapping('ChineseFont', 1, 0, 'ChineseFont')  # bold
                chinese_font = 'ChineseFont'
            else:
                chinese_font = 'Helvetica'
                logger.warning("æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“")
        except Exception as e:
            chinese_font = 'Helvetica'
            logger.warning(f"å­—ä½“æ³¨å†Œå¤±è´¥: {e}")
        
        # åˆ›å»ºPDFæ–‡æ¡£
        doc = SimpleDocTemplate(str(report_path), pagesize=A4)
        styles = getSampleStyleSheet()
        story = []
        
        # æ ‡é¢˜
        title_style = ParagraphStyle(
            'CustomTitle',
            parent=styles['Heading1'],
            fontSize=18,
            textColor=colors.darkblue,
            spaceAfter=30,
            alignment=1,  # å±…ä¸­å¯¹é½
            fontName=chinese_font
        )
        story.append(Paragraph("è§†é¢‘åˆ†ææŠ¥å‘Š", title_style))
        story.append(Spacer(1, 20))
        
        # è‡ªå®šä¹‰æ ·å¼
        heading2_style = ParagraphStyle(
            'CustomHeading2',
            parent=styles['Heading2'],
            fontName=chinese_font
        )
        normal_style = ParagraphStyle(
            'CustomNormal',
            parent=styles['Normal'],
            fontName=chinese_font
        )
        heading3_style = ParagraphStyle(
            'CustomHeading3',
            parent=styles['Heading3'],
            fontName=chinese_font
        )
        
        # åŸºæœ¬ä¿¡æ¯
        story.append(Paragraph("åŸºæœ¬ä¿¡æ¯", heading2_style))
        video_info = results.get("video_info", {})
        
        basic_info = [
            ["è§†é¢‘æ–‡ä»¶", video_path.name],
            ["åˆ†ææ—¶é—´", results.get("analysis_time", "")[:19]],
            ["è§†é¢‘æ—¶é•¿", f"{video_info.get('duration', 0):.2f} ç§’"],
            ["åˆ†è¾¨ç‡", f"{video_info.get('width', 0)} x {video_info.get('height', 0)}"],
            ["å¸§ç‡", f"{video_info.get('fps', 0):.2f} FPS"]
        ]
        
        info_table = Table(basic_info, colWidths=[2*inch, 3*inch])
        info_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
            ('FONTNAME', (0, 0), (-1, -1), chinese_font),
            ('FONTSIZE', (0, 0), (-1, 0), 14),
            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
            ('GRID', (0, 0), (-1, -1), 1, colors.black)
        ]))
        story.append(info_table)
        story.append(Spacer(1, 20))
        
        # åœºæ™¯åˆ†å‰²ç»“æœ
        segments = results.get("segments", [])
        if segments:
            story.append(Paragraph("åœºæ™¯åˆ†å‰²ç»“æœ", heading2_style))
            story.append(Paragraph(f"å…±è¯†åˆ« {len(segments)} ä¸ªåœºæ™¯", normal_style))
            story.append(Spacer(1, 10))
            
            # åŸºç¡€ä¿¡æ¯è¡¨æ ¼
            segment_data = [["åœºæ™¯", "å¼€å§‹æ—¶é—´", "ç»“æŸæ—¶é—´", "æ—¶é•¿", "æ„å›¾ç‰¹ç‚¹"]]
            for segment in segments[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                composition = segment.get('composition_analysis', 'æ ‡å‡†æ„å›¾')
                # æˆªå–æ„å›¾åˆ†æçš„å‰20ä¸ªå­—ç¬¦
                short_composition = composition[:20] + "..." if len(composition) > 20 else composition
                
                segment_data.append([
                    f"åœºæ™¯ {segment['segment_id']}",
                    f"{segment['start_time']:.2f}s",
                    f"{segment['end_time']:.2f}s",
                    f"{segment['duration']:.2f}s",
                    short_composition
                ])
            
            segment_table = Table(segment_data, colWidths=[0.8*inch, 1*inch, 1*inch, 0.8*inch, 2.4*inch])
            segment_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, -1), chinese_font),
                ('FONTSIZE', (0, 0), (-1, 0), 10),
                ('FONTSIZE', (0, 1), (-1, -1), 8),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.lightblue),
                ('GRID', (0, 0), (-1, -1), 1, colors.black),
                ('VALIGN', (0, 0), (-1, -1), 'MIDDLE')
            ]))
            story.append(segment_table)
            
            if len(segments) > 10:
                story.append(Paragraph(f"æ³¨ï¼šä»…æ˜¾ç¤ºå‰10ä¸ªåœºæ™¯ï¼Œå…±æœ‰{len(segments)}ä¸ªåœºæ™¯", normal_style))
            
            story.append(Spacer(1, 15))
            
            # è¯¦ç»†åˆ†æï¼ˆé€‰æ‹©å‰3ä¸ªåœºæ™¯ï¼‰
            story.append(Paragraph("é‡ç‚¹åœºæ™¯è¯¦ç»†åˆ†æ", heading2_style))
            for i, segment in enumerate(segments[:3]):
                story.append(Paragraph(f"åœºæ™¯ {segment['segment_id']} è¯¦ç»†åˆ†æ", heading3_style))
                
                detail_data = [
                    ["æ„å›¾åˆ†æ", segment.get('composition_analysis', 'æš‚æ— åˆ†æ')],
                    ["è¿é•œåˆ†æ", segment.get('camera_movement', 'æš‚æ— åˆ†æ')],
                    ["ä¸»é¢˜åˆ†æ", segment.get('theme_analysis', 'æš‚æ— åˆ†æ')],
                    ["ä¸“ä¸šç®€è¯„", segment.get('critical_review', 'æš‚æ— ç®€è¯„')]
                ]
                
                detail_table = Table(detail_data, colWidths=[1.5*inch, 4.5*inch])
                detail_table.setStyle(TableStyle([
                    ('BACKGROUND', (0, 0), (0, -1), colors.lightgrey),
                    ('FONTNAME', (0, 0), (-1, -1), chinese_font),
                    ('FONTSIZE', (0, 0), (-1, -1), 9),
                    ('ALIGN', (0, 0), (0, -1), 'CENTER'),
                    ('ALIGN', (1, 0), (1, -1), 'LEFT'),
                    ('VALIGN', (0, 0), (-1, -1), 'TOP'),
                    ('GRID', (0, 0), (-1, -1), 1, colors.black),
                    ('LEFTPADDING', (1, 0), (1, -1), 6),
                    ('RIGHTPADDING', (1, 0), (1, -1), 6)
                ]))
                story.append(detail_table)
                story.append(Spacer(1, 12))
            
            story.append(Spacer(1, 15))
        
        # è½¬åœºæ£€æµ‹ç»“æœ
        transitions = results.get("transitions", [])
        if transitions:
            story.append(Paragraph("è½¬åœºæ£€æµ‹ç»“æœ", heading2_style))
            story.append(Paragraph(f"å…±æ£€æµ‹åˆ° {len(transitions)} ä¸ªè½¬åœº", normal_style))
            story.append(Spacer(1, 10))
            
            transition_data = [["è½¬åœº", "æ—¶é—´ç‚¹", "å¼ºåº¦", "ç±»å‹"]]
            for transition in transitions[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                transition_data.append([
                    f"è½¬åœº {transition['transition_id']}",
                    f"{transition['timestamp']:.2f}s",
                    f"{transition['strength']:.3f}",
                    transition['type']
                ])
            
            transition_table = Table(transition_data, colWidths=[1*inch, 1.5*inch, 1*inch, 1.5*inch])
            transition_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, -1), chinese_font),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.lightgreen),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            story.append(transition_table)
            
            if len(transitions) > 10:
                story.append(Paragraph(f"æ³¨ï¼šä»…æ˜¾ç¤ºå‰10ä¸ªè½¬åœºï¼Œå…±æœ‰{len(transitions)}ä¸ª", normal_style))
            story.append(Spacer(1, 20))
        
        # è„šæœ¬å†…å®¹
        script_content = results.get("script_content", "")
        if script_content:
            story.append(Paragraph("è„šæœ¬å†…å®¹æ€»ç»“", heading2_style))
            
            # æå–è„šæœ¬ä¸­çš„æ€»ä½“è¯„ä»·éƒ¨åˆ†
            if "## æ€»ä½“è¯„ä»·" in script_content:
                summary_start = script_content.find("## æ€»ä½“è¯„ä»·") + len("## æ€»ä½“è¯„ä»·")
                summary_content = script_content[summary_start:].strip()
                # ç§»é™¤Markdownæ ¼å¼
                summary_content = summary_content.replace("**", "").replace("*", "")
                
                story.append(Paragraph("æ€»ä½“è¯„ä»·:", heading3_style))
                # åˆ†æ®µæ˜¾ç¤º
                paragraphs = summary_content.split('\n\n')
                for para in paragraphs[:3]:  # åªæ˜¾ç¤ºå‰3æ®µ
                    if para.strip():
                        story.append(Paragraph(para.strip(), normal_style))
                        story.append(Spacer(1, 6))
            
            story.append(Spacer(1, 10))
            story.append(Paragraph("å®Œæ•´è„šæœ¬è¯·æŸ¥çœ‹å•ç‹¬çš„è„šæœ¬æ–‡ä»¶", normal_style))
        
        # ç”ŸæˆPDF
        doc.build(story)
        logger.info(f"åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
        return report_path
    
    def _generate_segment_thumbnail(self, cap, start_time: float, fps: float, task_id: str, segment_id: int) -> str:
        """ç”Ÿæˆç‰‡æ®µç¼©ç•¥å›¾"""
        try:
            # å®šä½åˆ°æŒ‡å®šæ—¶é—´çš„ä¸­é—´å¸§
            frame_number = int(start_time * fps)
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
            
            ret, frame = cap.read()
            if not ret:
                return None
            
            # ç”Ÿæˆç¼©ç•¥å›¾æ–‡ä»¶å
            thumbnail_filename = f"{task_id}_segment_{segment_id}_thumbnail.jpg"
            thumbnail_path = self.output_dir / thumbnail_filename
            
            # è°ƒæ•´å›¾ç‰‡å¤§å°åˆ°åˆé€‚çš„å°ºå¯¸ï¼ˆå®½åº¦200pxï¼‰
            height, width = frame.shape[:2]
            new_width = 200
            new_height = int(height * (new_width / width))
            resized_frame = cv2.resize(frame, (new_width, new_height))
            
            # ä¿å­˜ç¼©ç•¥å›¾
            cv2.imwrite(str(thumbnail_path), resized_frame)
            logger.info(f"ç”Ÿæˆç¼©ç•¥å›¾: {thumbnail_path}")
            
            # è¿”å›URLè·¯å¾„
            return f"/uploads/{thumbnail_filename}"
        
        except Exception as e:
            logger.error(f"ç”Ÿæˆç¼©ç•¥å›¾å¤±è´¥: {e}")
            return None
    
    def _generate_segment_gif(self, video_path: Path, start_time: float, end_time: float, task_id: str, segment_id: int) -> str:
        """ç”Ÿæˆç‰‡æ®µGIFåŠ¨ç”»"""
        try:
            import subprocess
            import shutil
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ffmpeg
            if not shutil.which("ffmpeg"):
                logger.warning("FFmpegä¸å¯ç”¨ï¼Œè·³è¿‡GIFç”Ÿæˆ")
                return None
            
            # é™åˆ¶GIFæ—¶é•¿ï¼ˆæœ€å¤š5ç§’ï¼‰å’Œå¤§å°
            duration = min(end_time - start_time, 5.0)
            
            # ç”ŸæˆGIFæ–‡ä»¶å
            gif_filename = f"{task_id}_segment_{segment_id}.gif"
            gif_path = self.output_dir / gif_filename
            
            # ä½¿ç”¨FFmpegç”ŸæˆGIF
            cmd = [
                "ffmpeg", "-y",  # è¦†ç›–è¾“å‡ºæ–‡ä»¶
                "-ss", str(start_time),  # å¼€å§‹æ—¶é—´
                "-i", str(video_path),  # è¾“å…¥è§†é¢‘
                "-t", str(duration),  # æŒç»­æ—¶é—´
                "-vf", "scale=320:-1:flags=lanczos,fps=10",  # ç¼©æ”¾å’Œå¸§ç‡
                "-loop", "0",  # æ— é™å¾ªç¯
                str(gif_path)
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0 and gif_path.exists():
                logger.info(f"ç”ŸæˆGIF: {gif_path}")
                return f"/uploads/{gif_filename}"
            else:
                logger.warning(f"FFmpegç”ŸæˆGIFå¤±è´¥: {result.stderr}")
                return None
        
        except subprocess.TimeoutExpired:
            logger.warning(f"ç”Ÿæˆç‰‡æ®µ{segment_id}GIFè¶…æ—¶")
            return None
        except Exception as e:
            logger.error(f"ç”Ÿæˆç‰‡æ®µ{segment_id}GIFå¤±è´¥: {e}")
            return None

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•°
def test_video_analyzer():
    """æµ‹è¯•è§†é¢‘åˆ†æå™¨"""
    analyzer = VideoAnalyzer()
    
    # æµ‹è¯•é…ç½®
    test_config = {
        "video_segmentation": True,
        "transition_detection": True,
        "audio_transcription": True,
        "report_generation": True
    }
    
    def progress_callback(progress, message):
        print(f"è¿›åº¦: {progress}% - {message}")
    
    # è¿™é‡Œéœ€è¦æä¾›ä¸€ä¸ªå®é™…çš„è§†é¢‘æ–‡ä»¶è·¯å¾„è¿›è¡Œæµ‹è¯•
    # video_path = "test_video.mp4"
    # results = analyzer.analyze_video(video_path, test_config, progress_callback)
    # print("åˆ†æå®Œæˆ:", results)

if __name__ == "__main__":
    test_video_analyzer() 