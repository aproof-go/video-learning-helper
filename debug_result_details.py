#!/usr/bin/env python3
import json

with open('video-learning-helper-backend/uploads/75e7c02d-144f-4be3-beb1-e8b643c60e78_results.json') as f:
    data = json.load(f)
    
print('ğŸ” ç»“æœæ–‡ä»¶è¯¦ç»†åˆ†æ:')
print('=' * 50)
print('è§†é¢‘è·¯å¾„:', data.get('video_path', 'N/A'))
print('åˆ†ææ—¶é—´:', data.get('analysis_time', 'N/A'))
print('ç‰‡æ®µç”Ÿæˆå™¨:', data.get('generator', 'N/A'))

segments = data.get('segments', [])
print(f'\nğŸ“Š ç‰‡æ®µåˆ†æ ({len(segments)} ä¸ª):')
print('ç‰‡æ®µç±»å‹åˆ†å¸ƒ:', {s['scene_type'] for s in segments})
print('ç‰‡æ®µæ€»æ•°:', len(segments))
print('å‰10ä¸ªç‰‡æ®µæ—¶é•¿:', [s['duration'] for s in segments][:10])

if segments:
    scene_types = {}
    for s in segments:
        scene_type = s.get('scene_type', 'æœªçŸ¥')
        scene_types[scene_type] = scene_types.get(scene_type, 0) + 1
    
    print('\nåœºæ™¯ç±»å‹ç»Ÿè®¡:')
    for scene_type, count in scene_types.items():
        print(f'  {scene_type}: {count} ä¸ª')

print('\nğŸï¸ è½¬åœºåˆ†æ:')
transitions = data.get('transitions', [])
print('è½¬åœºæ€»æ•°:', len(transitions))
if transitions:
    print('å‰3ä¸ªè½¬åœº:', [(t['timestamp'], t['strength'], t['type']) for t in transitions[:3]])

print('\nğŸ™ï¸ è½¬å½•åˆ†æ:')
transcription = data.get('transcription', {})
if transcription:
    print('è½¬å½•æ–‡æœ¬é•¿åº¦:', len(transcription.get('text', '')))
    print('è½¬å½•ç‰‡æ®µæ•°:', len(transcription.get('segments', [])))
else:
    print('æ— è½¬å½•æ•°æ®') 